# ğŸ§  Clustering & DBSCAN â€” A Learning Journey  

**Author:** Yazdan Ghanavati  
ğŸ“ Master's Student, ICT for Internet and Multimedia â€” University of Padova  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/yazdan-ghanavati/) â€¢ ğŸ’» [GitHub](https://github.com/Yazdan-Ghanavati)  

---

## ğŸ§¾ Overview  

This repository documents my **open learning journey** into clustering algorithms â€” starting from basic intuition to the practical implementation of **K-Means** and **DBSCAN**.  
The goal is not only to build working code, but to **teach through exploration**, helping others understand the concepts step by step.  

Each concept is implemented in a single Jupyter Notebook (`DBSCAN_mini_series.ipynb`), with detailed markdown explanations and saved visuals inside the `/imgs` folder.


---

## ğŸ§© Project Roadmap  

| # | Section | Status | Focus |
|---|----------|--------|--------|
| 1 | Why Clustering? | âœ… | Understanding why we group unlabeled data |
| 2 | K-Means Basics | âœ… | How K-Means forms clusters iteratively |
| 3 | K-Means Failures | â¬œ | Why K-Means struggles on complex data |
| 4 | DBSCAN Intuition | â¬œ | Core, border, and noise points visually explained |
| 5 | Cluster Expansion | â¬œ | How DBSCAN grows clusters from dense regions |
| 6 | Tuning Îµ & MinPts | â¬œ | Finding the right parameters |
| 7 | DBSCAN vs K-Means | â¬œ | Comparing both approaches and summarizing insights |

----

## ğŸ§© Results & Reflection  


### 1ï¸âƒ£ Why Clustering?  

The first part of this project focuses on a simple but fundamental question:  
**why do we cluster data?**

Clustering is the process of grouping similar data points when no labels are provided â€”  
it helps us reveal hidden structure and meaning inside complex datasets.  

To visualize this idea, I generated a random dataset with four hidden groups.  
Below, the **left plot** shows how the raw, unlabeled data initially looks like random noise.  
The **right plot**, after applying a simple clustering algorithm, reveals the underlying groups â€”  
showing how unsupervised learning can transform chaos into insight.  

<p align="center">
  <img src="imgs/why_clustering.png" width="700">
</p>

This simple example demonstrates the beauty of clustering:  
without knowing any labels, we can still discover meaningful organization in the data.  
Thatâ€™s the foundation of many modern applications â€”  
from customer segmentation and image recognition to medical pattern discovery.  

In the next stage, weâ€™ll dive deeper into **K-Means**,  
understanding exactly *how* it finds these clusters, and *why* it sometimes fails â€”  
paving the way toward more flexible methods like **DBSCAN**.

---

### 2ï¸âƒ£ K-Means Basics  

Now that we know *why* clustering matters, letâ€™s look at one of the simplest yet most influential algorithms â€” **K-Means**.  

K-Means groups data by minimizing the distance between each point and its assigned cluster centroid. The â€œKâ€ simply refers to the **number of clusters (or centroids)** you choose.  

Hereâ€™s what happens step by step:  
1. ğŸ² **Random Initialization:** K random centroids are placed in the dataset.  
2. ğŸ¯ **Assignment Step:** Each point is assigned to the closest centroid.  
3. ğŸ” **Update Step:** Each centroid moves to the average position of its assigned points.  
4. The process repeats until centroids stop moving â€” *thatâ€™s convergence*.  

The GIF below demonstrates this process visually. You can see random initialization, cluster assignment, and centroid movement until stability.  

<p align="center">
  <img src="imgs/kmeans_process.gif" width="700">
</p>

And hereâ€™s the final result after convergence ğŸ‘‡  

<p align="center">
  <img src="imgs/kmeans_final.png" width="700">
</p>

Even though K-Means often finds meaningful patterns, it relies heavily on **the initial random placement** of centroids â€” sometimes leading to suboptimal clustering.  

ğŸ“˜ *P.S.*  
In the previous section, we used scikit-learnâ€™s `KMeans` for a quick demo.  
Here, we implemented it manually to visualize the inner workings â€” initialization, assignment, and centroid updates â€”  
which arenâ€™t visible when using the library version. This deeper look shows *how* K-Means actually converges.  

In the next part, weâ€™ll explore when K-Means fails and why density-based algorithms like **DBSCAN** handle complex shapes and noise more effectively.  

---
---

### 3ï¸âƒ£ When K-Means Fails  

While K-Means is simple and intuitive, it assumes that clusters are **spherical**, of **equal size**, and **uniform density**.  
When those assumptions break, so does K-Means.

In this section, we visualized three classic failure cases:

- **Non-spherical shapes:** linear boundaries cut through curved clusters (e.g., â€œtwo moonsâ€).  
- **Varying density:** sparse clusters get absorbed or split by denser ones.  
- **Noise/outliers:** centroids are pulled toward random points, sometimes forming fake clusters.

This comparison highlights that K-Means doesnâ€™t â€œunderstandâ€ data distribution â€” it only minimizes distance to centroids.  
In the next stage, weâ€™ll meet **DBSCAN**, which overcomes these limitations by clustering based on *density* and identifying *noise* naturally.

<p align="center">
  <img src="imgs/kmeans_failures_pairs.png" width="800">
</p>

---






## âš™ï¸ How to Run  

1. **Clone the repository**  
   Use Git to copy the project locally and move into its folder:  

   git clone https://github.com/<your-username>/clustering-dbscan-mini.git  
   cd clustering-dbscan-mini  

2. **Create and activate a virtual environment**  
   Itâ€™s best practice to isolate dependencies:  

   â€¢ macOS / Linux â†’ `python -m venv .venv && source .venv/bin/activate`  
   â€¢ Windows â†’ `python -m venv .venv && .venv\Scripts\activate`  

3. **Install dependencies**  
   Install all required libraries listed in `requirements.txt`:  

   pip install -r requirements.txt  

4. **Run the notebook**  
   Launch Jupyter and open the main notebook:  

   jupyter notebook DBSCAN_mini_series.ipynb  

ğŸ–¼ï¸ Figures will be saved automatically to the `/imgs` folder after each run.


---


## ğŸ§  Notes  

- This notebook is being updated progressively as new sections are completed.  
- Each section corresponds to a short **LinkedIn post** that explains clustering intuitively.  
- The repository serves as both a **learning resource** and a **portfolio artifact** demonstrating professional documentation and reproducible code.  

---

## ğŸ§¾ How to Cite This Work  

If you find this notebook useful or wish to reference it in your own work, please cite it as follows:  

**Yazdan Ghanavati.** *Clustering & DBSCAN â€” A Learning Journey.*  
GitHub Repository, 2025. Available at: https://github.com/<your-username>/clustering-dbscan-mini  

You may also include the following BibTeX entry for academic use:  

@misc{ghanavati2025clustering,  
  author       = {Yazdan Ghanavati},  
  title        = {Clustering & DBSCAN â€” A Learning Journey},  
  year         = {2025},  
  publisher    = {GitHub},  
  journal      = {GitHub repository},  
  howpublished = {\url{https://github.com/<your-username>/clustering-dbscan-mini}},  
  note         = {Open learning project on clustering algorithms}  
}  

---

ğŸ“© *If you found this helpful, feel free to connect or share your feedback on LinkedIn â€” learning is always better together!* ğŸš€


---
